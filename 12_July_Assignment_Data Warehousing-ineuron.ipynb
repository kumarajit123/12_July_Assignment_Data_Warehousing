{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba51909",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC: ETL and Data Integration\n",
    "  1. Design an ETL process using a programming language (e.g., Python) to extract data from a source system (e.g., CSV files), transform it by applying certain business rules or calculations, and load it into a data warehouse.\n",
    "  2. Implement the ETL process by writing code that performs the extraction, transformation, and loading steps.\n",
    "\n",
    "\n",
    "Solution:-  Python ETL Example\n",
    "ETL is the process of extracting a huge amount of data from a wide array of sources and formats and then converting & consolidating it into a single format before storing it in a database or writing it to a destination file.\n",
    "\n",
    "In this example, some of the data is stored in CSV files while others are in JSON files. All of this data has to be consolidated into a single format and then stored in a unified file location.\n",
    "\n",
    "Step 1: Import the modules and functions\n",
    "In this ETL using Python example, first, you need to import the required modules and functions.\n",
    "\n",
    "\n",
    "import glob \n",
    "import pandas as pd \n",
    "import xml.etree.ElementTree as ET \n",
    "from datetime import datetime\n",
    "The dealership_data file contains CSV, JSON, and XML files for used car data. The features incorporated here are car_model, year_of_manufacture, price, and fuel. So you need to extract the file from the raw data, transform it into a target file, and then load it into the output.\n",
    "Download the source file:\n",
    "\n",
    "\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0221EN-SkillsNetwork/labs/module%206/Lab%20-%20Extract%20Transform%20Load/data/datasource.zip\n",
    "Extracting the zip file:\n",
    "\n",
    "\n",
    "nzip datasource.zip -d dealership_data\n",
    "Setting the path for Target files:\n",
    "\n",
    "\n",
    "tmpfile    = \"dealership_temp.tmp\"               # store all extracted data\n",
    "\n",
    "logfile    = \"dealership_logfile.txt\"            # all event logs will be stored\n",
    "\n",
    "targetfile = \"dealership_transformed_data.csv\"   # transformed data is stored\n",
    "Step 2: Extract\n",
    "The Extract function in this ETL using Python example is used to extract a huge amount of data in batches. This data is extracted from numerous sources.\n",
    "\n",
    "CSV Extract Function:\n",
    "\n",
    "\n",
    "def extract_from_csv(file_to_process): \n",
    "    dataframe = pd.read_csv(file_to_process) \n",
    "    return dataframe\n",
    "JSON Extract Function\n",
    "\n",
    "\n",
    "def extract_from_json(file_to_process):\n",
    "    dataframe = pd.read_json(file_to_process,lines=True)\n",
    "    return dataframe\n",
    "XML Extract Function\n",
    "\n",
    "\n",
    "def extract_from_xml(file_to_process):\n",
    "\n",
    "    dataframe = pd.DataFrame(columns=['car_model','year_of_manufacture','price', 'fuel'])\n",
    "\n",
    "    tree = ET.parse(file_to_process) \n",
    "\n",
    "    root = tree.getroot() \n",
    "\n",
    "    for person in root: \n",
    "\n",
    "        car_model = person.find(\"car_model\").text \n",
    "\n",
    "        year_of_manufacture = int(person.find(\"year_of_manufacture\").text)\n",
    "\n",
    "        price = float(person.find(\"price\").text) \n",
    "\n",
    "        fuel = person.find(\"fuel\").text \n",
    "\n",
    "        dataframe = dataframe.append({\"car_model\":car_model, \"year_of_manufacture\":year_of_manufacture, \"price\":price, \"fuel\":fuel}, ignore_index=True) \n",
    "\n",
    "        return dataframe\n",
    "Calling Extract Function()\n",
    "\n",
    "\n",
    "def extract():\n",
    "       extracted_data = pd.DataFrame(columns=['car_model','year_of_manufacture','price', 'fuel']) \n",
    "    #for csv files\n",
    "      for csvfile in glob.glob(\"dealership_data/*.csv\"):\n",
    "          extracted_data = extracted_data.append(extract_from_csv(csvfile), ignore_index=True)\n",
    "    #for json files\n",
    "      for jsonfile in glob.glob(\"dealership_data/*.json\"):\n",
    "          extracted_data = extracted_data.append(extract_from_json(jsonfile), ignore_index=True)\n",
    "    #for xml files\n",
    "      for xmlfile in glob.glob(\"dealership_data/*.xml\"):\n",
    "          extracted_data = extracted_data.append(extract_from_xml(xmlfile), ignore_index=True)\n",
    "      return extracted_data\n",
    "Step 3: Transform\n",
    "Using the transform function you can convert the data in any format as per your needs.\n",
    "\n",
    "\n",
    "def transform(data):\n",
    "       data['price'] = round(data.price, 2)\n",
    "       return data\n",
    "Step 4: Loading and Logging\n",
    "In this step, the data is loaded to the destination file. A logging entry needs to be established before loading.\n",
    "\n",
    "load function()\n",
    "\n",
    "\n",
    "def load(targetfile,data_to_load):\n",
    "    data_to_load.to_csv(targetfile)\n",
    "log function()\n",
    "\n",
    "\n",
    "def log(message):\n",
    "    timestamp_format = '%H:%M:%S-%h-%d-%Y'\n",
    "    #Hour-Minute-Second-MonthName-Day-Year\n",
    "    now = datetime.now() # get current timestamp\n",
    "    timestamp = now.strftime(timestamp_format)\n",
    "    with open(\"dealership_logfile.txt\",\"a\") as f: f.write(timestamp + ',' + message + 'n')\n",
    "Step 5: Running ETL Process\n",
    "The log indicates that you have started the ETL process.\n",
    "\n",
    "\n",
    "log(\"ETL Job Started\")\n",
    "The log indicates that you have started and ended the Extract phase.\n",
    "\n",
    "\n",
    "log(\"Extract phase Started\")\n",
    "extracted_data = extract() \n",
    "log(\"Extract phase Ended\")\n",
    "The log indicates that you have started and ended the Transform phase.\n",
    "\n",
    "\n",
    "log(“Transform phase Started”)\n",
    "transformed_data = transform(extracted_data)\n",
    "log(\"Transform phase Ended\")\n",
    "The log indicates that you have started and ended the Load phase.\n",
    "\n",
    "\n",
    "log(\"Load phase Started\")\n",
    "load(targetfile,transformed_data)\n",
    "log(\"Load phase Ended\")\n",
    "The log indicates that the ETL process has ended.\n",
    "\n",
    "\n",
    "log(\"ETL Job Ended\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb05c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC: Performance Optimization and Querying\n",
    "    1. Scenario: You need to improve the performance of your data loading process in the data warehouse. Write a Python script that implements the following optimizations:\n",
    "Utilize batch processing techniques to load data in bulk instead of individual row insertion.\n",
    "      b)  Implement multi-threading or multiprocessing to parallelize the data loading process.\n",
    "      c)  Measure the time taken to load a specific amount of data before and after implementing these optimizations.\n",
    "\n",
    "Solution:- Data acquisition is a large part of many data analytics projects and system development life cycles. \n",
    "    This article will show you how to write a simple Python program that uses the BULK INSERT utility to rapidly insert data from a CSV file into a SQL Server database table.\n",
    "\n",
    "Why use this Approach?\n",
    "There are many ways to load data from a CSV file into a SQL Server table. Here a few methods:\n",
    "\n",
    "Run the BULK INSERT utility from the command line.\n",
    "Run the BULK INSERT utility from SQL Server Management Studio (SSMS).\n",
    "Use the SQL Server Management Studio (SSMS) Import Flat File wizard.\n",
    "Write a program that opens the CSV file, reads its records one-by-one, and calls a SQL INSERT statement to insert the rows into a database table.\n",
    "If there are so many ways to get data from a CSV into a SQL Server database, why write a Python program that calls the BULK INSERT utility to load it into a table? Here are some reasons why this might be a helpful approach:\n",
    "\n",
    "The Python program could perform process steps before it executes BULK INSERT.\n",
    "The Python program could ensure that it does not write duplicate data to the destination table.\n",
    "The program could cleanse or transform the data following the BULK INSERT.\n",
    "It could perform error-handling functions.\n",
    "It could send notifications, by email or other methods, about its actions.\n",
    "\n",
    "Creating the Database and Table\n",
    "Important Note about SQL Server Versions\n",
    "Microsoft introduced the ability to use BULK INSERT to insert data from CSV files in SQL Server 2017. So, you will need that version or newer to use this capability.\n",
    "\n",
    "Create the Database\n",
    "Here are the steps used to create a database called HR (for Human Resources):\n",
    "\n",
    "Connect to SQL Server\n",
    "\n",
    "Launch SSMS.\n",
    "Connect to the database server. In this case, I used Windows authentication to connect to the locally-installed instance of SQL Server Express.\n",
    "\n",
    "\n",
    "Create the HR Database\n",
    "\n",
    "Expand the [+ Databases] node in Object Explorer. Right-click on [+ Database] and click on [New Database…].\n",
    "In the New Database dialog box, enter “HR” into the Database name textbox. Leave all settings as is. Click on [OK] to create the database.\n",
    "\n",
    "Verify that the HR database appears in Object Explorer. If not, right-click on Databases and click on [Refresh]. It should come into view.\n",
    "\n",
    "Create the Table\n",
    "At this point, the HR database will not contain any tables or other objects, such as stored procedures. While BULK INSERT can create tables when it runs, I have found that creating tables ahead of time offers advantages. For example, I can specify the table’s key column(s) and each column’s type and length. Looking again at the sample data in the CSV file, let’s create columns with the same names and with these data types:\n",
    "\n",
    "ID — INT\n",
    "Job Title — NCHAR(60)\n",
    "Email Address — NCHAR(120)\n",
    "FirstName LastName — NCHAR(80)\n",
    "Since all rows will have values for all columns, set each column to NOT NULL. Also, since ID is a unique identifier for each row, select it as the key.\n",
    "\n",
    "Follow these steps to create the table:\n",
    "\n",
    "In Object Explorer, click on [+ HR] to view the selection within the database.\n",
    "Right-click on [+ Tables] and click on [New] and then click on [Table…].\n",
    "Enter the data as shown below. After the Column Name, Data Type, and Allow Nulls values have been entered, right-click on the ID column name and click on [Set Primary Key]. Setting ID as a key will ensure that only one row in the table can contain any ID value.\n",
    "\n",
    "\n",
    "Click on the Save icon in the ribbon menu, and in the Choose Name dialog box, enter the name “Person.” Click on [OK] to save the table.\n",
    "In Object Explorer, click on [+ Tables] to expand the node. Then, right-click on [- Tables] and click on [Refresh]. The Person table should now be in view.\n",
    "Click on [+ dbo.Person] and then on [+ Columns] to examine the table’s structure. \n",
    "\n",
    "The Python Program\n",
    "Now that the HR database and Person table exist let’s examine a simple Python program that uses the BULK INSERT utility. It simply inserts all records from the CSV file into the Person table.\n",
    "\n",
    "Code Modules\n",
    "This Python program consists of two modules or files:\n",
    "\n",
    "c_bulk_insert.py contains the c_bulk_insert class. It includes functions to connect to the database and build and execute a BULK INSERT statement to insert data from a CSV file into a database table.\n",
    "sql_server_bulk_insert.py simply instantiates the c_bulk_insert class and calls it with the information needed to do its work.\n",
    "Code Logic\n",
    "When the program instantiates class c_bulk_insert, it performs these steps:\n",
    "\n",
    "Connect to the SQL Server database.\n",
    "Construct the BULK INSERT query with the destination table’s name, input CSV file, and some settings.\n",
    "Open a database cursor.\n",
    "Execute the query.\n",
    "Clean up: Commit the BULK INSERT transactions, close the cursor, and close the database connection.\n",
    "The Code\n",
    "The Python class c_bulk_insert in module c_bulk_insert.py performs the logic described in the Code Logic section above.\n",
    "\n",
    "\"\"\" \n",
    "    Name:           c_bulk_insert.py\n",
    "    Author:         Randy Runtsch\n",
    "    Date:           March 17, 2021\n",
    "    Description:    This module contains the c_bulk_insert class that connect to a SQL Server database\n",
    "                    and executes the BULK INSERT utility to insert data from a CSV file into a table.\n",
    "    Prerequisites:  1. Create the database data table.\n",
    "                    2. Create the database update_csv_log table.\n",
    "\"\"\"\n",
    "import pyodbc\n",
    "class c_bulk_insert:\n",
    "def __init__(self, csv_file_nm, sql_server_nm, db_nm, db_table_nm):\n",
    "# Connect to the database, perform the insert, and update the log table.\n",
    "conn = self.connect_db(sql_server_nm, db_nm)\n",
    "        self.insert_data(conn, csv_file_nm, db_table_nm)\n",
    "        conn.close\n",
    "def connect_db(self, sql_server_nm, db_nm):\n",
    "# Connect to the server and database with Windows authentication.\n",
    "conn_string = 'DRIVER={ODBC Driver 17 for SQL Server};SERVER=' + sql_server_nm + ';DATABASE=' + db_nm + ';Trusted_Connection=yes;'\n",
    "        conn = pyodbc.connect(conn_string)\n",
    "return conn\n",
    "def insert_data(self, conn, csv_file_nm, db_table_nm):\n",
    "# Insert the data from the CSV file into the database table.\n",
    "# Assemble the BULK INSERT query. Be sure to skip the header row by specifying FIRSTROW = 2.\n",
    "qry = \"BULK INSERT \" + db_table_nm + \" FROM '\" + csv_file_nm + \"' WITH (FORMAT = 'CSV', FIRSTROW = 2)\"\n",
    "# Execute the query\n",
    "cursor = conn.cursor()\n",
    "        success = cursor.execute(qry)\n",
    "        conn.commit()\n",
    "        cursor.close\n",
    "The module sql_server_bulk_insert.py instantiates c_bulk_insert. It calls it with:\n",
    "\n",
    "CSV file name\n",
    "SQL Server instance engine name\n",
    "Database name\n",
    "Destination table name\n",
    "\"\"\" \n",
    "    Name:           sql_server_bulk_insert.py\n",
    "    Author:         Randy Runtsch\n",
    "    Date:           March 17, 2021\n",
    "    Description:    This program is the controller that uses the Microsoft Transact-SQL BULK INSERT\n",
    "                    statement to quickly insert the rows from a CSV file into\n",
    "                    a SQL Server table.\n",
    "    Prerequisites:  1. Create the database data table.\n",
    "                    2. Create the database update_csv_log table.\n",
    "\"\"\"\n",
    "from c_bulk_insert import c_bulk_insert\n",
    "bulk_insert = c_bulk_insert(r'c:\\\\test_data\\\\person.csv', 'xxxxx-DESKTOP-\\\\SQLEXPRESS', 'HR', 'Person')\n",
    "The Results\n",
    "After the program runs, executing a SELECT query in SSMS shows that it wrote the records from the CSV file to the Person table.\n",
    "\n",
    "Select Top(1000) [ID]\n",
    "     ,[Job Title]\n",
    "     ,[Email Address]\n",
    "     ,[Firstname Lastname]\n",
    "    From [HR].[dbo].[Person]\n",
    "\n",
    "\n",
    "Where to Go from Here\n",
    "There may be many reasons and ways to enhance the program. Here are a few ideas:\n",
    "\n",
    "Add error handling to the database connection, query execution, and other parts of the program. The program could use error handling, for example, to gracefully shut down, retry a set number of times, and notify the appropriate parties by email.\n",
    "Automate the program to insert data into the database when needed. For example, schedule the program to run periodically with Windows Task Scheduler to insert person records from new CSV files.\n",
    "Create and write to a log table to capture the program start and finish times and other important events. Add other messages, such as error details, to identify issues to troubleshoot.\n",
    "Conclusion\n",
    "As you can see, using Python to call BULK INSERT is a way to automate part of a workflow to quickly insert data from a CSV file into a SQL Server database table. It might prove to be a handy technique to add to your data analytics or software development toolkit.\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e2546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Python class c_bulk_insert in module c_bulk_insert.py performs the logic described in the Code Logic section above.\n",
    "\n",
    "\"\"\" \n",
    "    Name:           c_bulk_insert.py\n",
    "    Author:         Randy Runtsch\n",
    "    Date:           March 17, 2021\n",
    "    Description:    This module contains the c_bulk_insert class that connect to a SQL Server database\n",
    "                    and executes the BULK INSERT utility to insert data from a CSV file into a table.\n",
    "    Prerequisites:  1. Create the database data table.\n",
    "                    2. Create the database update_csv_log table.\n",
    "\"\"\"\n",
    "import pyodbc\n",
    "class c_bulk_insert:\n",
    "def __init__(self, csv_file_nm, sql_server_nm, db_nm, db_table_nm):\n",
    "# Connect to the database, perform the insert, and update the log table.\n",
    "conn = self.connect_db(sql_server_nm, db_nm)\n",
    "        self.insert_data(conn, csv_file_nm, db_table_nm)\n",
    "        conn.close\n",
    "def connect_db(self, sql_server_nm, db_nm):\n",
    "# Connect to the server and database with Windows authentication.\n",
    "conn_string = 'DRIVER={ODBC Driver 17 for SQL Server};SERVER=' + sql_server_nm + ';DATABASE=' + db_nm + ';Trusted_Connection=yes;'\n",
    "        conn = pyodbc.connect(conn_string)\n",
    "return conn\n",
    "def insert_data(self, conn, csv_file_nm, db_table_nm):\n",
    "# Insert the data from the CSV file into the database table.\n",
    "# Assemble the BULK INSERT query. Be sure to skip the header row by specifying FIRSTROW = 2.\n",
    "qry = \"BULK INSERT \" + db_table_nm + \" FROM '\" + csv_file_nm + \"' WITH (FORMAT = 'CSV', FIRSTROW = 2)\"\n",
    "# Execute the query\n",
    "cursor = conn.cursor()\n",
    "        success = cursor.execute(qry)\n",
    "        conn.commit()\n",
    "        cursor.close\n",
    "\n",
    "#The module sql_server_bulk_insert.py instantiates c_bulk_insert. It calls it with:\n",
    "#1)CSV file name\n",
    "#2)SQL Server instance engine name\n",
    "#3)Database name\n",
    "#4)Destination table name\n",
    "\"\"\" \n",
    "    Name:           sql_server_bulk_insert.py\n",
    "    Author:         Randy Runtsch\n",
    "    Date:           March 17, 2021\n",
    "    Description:    This program is the controller that uses the Microsoft Transact-SQL BULK INSERT\n",
    "                    statement to quickly insert the rows from a CSV file into\n",
    "                    a SQL Server table.\n",
    "    Prerequisites:  1. Create the database data table.\n",
    "                    2. Create the database update_csv_log table.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
